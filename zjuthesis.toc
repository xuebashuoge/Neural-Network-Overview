\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\contentsline {section}{\numberline {1}Introduction}{1}{section.0.0.1}% 
\contentsline {section}{\numberline {2}Neural Network}{2}{section.0.0.2}% 
\contentsline {subsection}{\numberline {2.1}Neuron}{2}{subsection.0.0.2.1}% 
\contentsline {subsection}{\numberline {2.2}Architecture}{3}{subsection.0.0.2.2}% 
\contentsline {subsection}{\numberline {2.3}Loss Function}{5}{subsection.0.0.2.3}% 
\contentsline {subsection}{\numberline {2.4}Gradient Descent}{6}{subsection.0.0.2.4}% 
\contentsline {subsubsection}{\numberline {2.4.1}Basic Intuition of GD}{6}{subsubsection.0.0.2.4.1}% 
\contentsline {subsubsection}{\numberline {2.4.2}Basic Convergence Analysis of GD}{6}{subsubsection.0.0.2.4.2}% 
\contentsline {subsection}{\numberline {2.5}Forward Propagation}{8}{subsection.0.0.2.5}% 
\contentsline {subsection}{\numberline {2.6}Backward Propagation}{8}{subsection.0.0.2.6}% 
\contentsline {section}{\numberline {3}Problem in Neural Network}{9}{section.0.0.3}% 
\contentsline {subsection}{\numberline {3.1}Explosion and Vanishing}{10}{subsection.0.0.3.1}% 
\contentsline {subsection}{\numberline {3.2}Saddle Points in Non-convex Optimization}{11}{subsection.0.0.3.2}% 
\contentsline {subsubsection}{\numberline {3.2.1}Hessian Matrix}{11}{subsubsection.0.0.3.2.1}% 
\contentsline {subsubsection}{\numberline {3.2.2}The Prevalence of Saddle Points}{12}{subsubsection.0.0.3.2.2}% 
\contentsline {subsection}{\numberline {3.3}Learning Rate Selection}{14}{subsection.0.0.3.3}% 
\contentsline {subsubsection}{\numberline {3.3.1}Goldstein Rule}{14}{subsubsection.0.0.3.3.1}% 
\contentsline {subsubsection}{\numberline {3.3.2}Armijo Rule}{15}{subsubsection.0.0.3.3.2}% 
\contentsline {subsubsection}{\numberline {3.3.3}Diminishing Learning Rate}{15}{subsubsection.0.0.3.3.3}% 
\contentsline {section}{\numberline {4}Tricks in Neural Network}{15}{section.0.0.4}% 
\contentsline {subsection}{\numberline {4.1}A Good Initialization is All You Need \footnote {The title of this section is quoted from Mishkin et al. \parencite {mishkin2015all} whose title had a profound influence on our study of the neural network.}}{15}{subsection.0.0.4.1}% 
\contentsline {subsubsection}{\numberline {4.1.1}Xavier Initialization}{16}{subsubsection.0.0.4.1.1}% 
\contentsline {subsubsection}{\numberline {4.1.2}Kaiming Initialization}{17}{subsubsection.0.0.4.1.2}% 
\contentsline {subsubsection}{\numberline {4.1.3}Orthogonal Initialization}{18}{subsubsection.0.0.4.1.3}% 
\contentsline {subsubsection}{\numberline {4.1.4}Layer-Sequential Unit-Variance Initialization (LSUV)}{19}{subsubsection.0.0.4.1.4}% 
\contentsline {subsection}{\numberline {4.2}Gradient Descend Variants}{20}{subsection.0.0.4.2}% 
\contentsline {subsubsection}{\numberline {4.2.1}Batch Gradient Descent}{20}{subsubsection.0.0.4.2.1}% 
\contentsline {subsubsection}{\numberline {4.2.2}Stochastic Gradient Descent (SGD)}{20}{subsubsection.0.0.4.2.2}% 
\contentsline {subsubsection}{\numberline {4.2.3}Mini-batch Gradient Descent}{21}{subsubsection.0.0.4.2.3}% 
\contentsline {subsection}{\numberline {4.3}Optimization Algorithms}{21}{subsection.0.0.4.3}% 
\contentsline {subsubsection}{\numberline {4.3.1}Momentum}{21}{subsubsection.0.0.4.3.1}% 
\contentsline {subsubsection}{\numberline {4.3.2}Nesterov Accelerated Gradient (NAG)}{23}{subsubsection.0.0.4.3.2}% 
\contentsline {subsubsection}{\numberline {4.3.3}RMSprop}{24}{subsubsection.0.0.4.3.3}% 
\contentsline {subsubsection}{\numberline {4.3.4}Adagrad}{25}{subsubsection.0.0.4.3.4}% 
\contentsline {subsubsection}{\numberline {4.3.5}Adadelta}{25}{subsubsection.0.0.4.3.5}% 
\contentsline {subsubsection}{\numberline {4.3.6}Adam}{26}{subsubsection.0.0.4.3.6}% 
\contentsline {subsubsection}{\numberline {4.3.7}AdaMax}{28}{subsubsection.0.0.4.3.7}% 
\contentsline {subsubsection}{\numberline {4.3.8}NAdam}{30}{subsubsection.0.0.4.3.8}% 
\contentsline {section}{\numberline {5}Main Questions in Neural Network Theory}{31}{section.0.0.5}% 
\contentsline {subsection}{\numberline {5.1}Representation Power of Deep Neural Networks}{32}{subsection.0.0.5.1}% 
\contentsline {subsubsection}{\numberline {5.1.1}Definition and Notation}{32}{subsubsection.0.0.5.1.1}% 
\contentsline {subsubsection}{\numberline {5.1.2}Shallow Networks}{32}{subsubsection.0.0.5.1.2}% 
\contentsline {subsubsection}{\numberline {5.1.3}Deep Hierarchy Networks}{33}{subsubsection.0.0.5.1.3}% 
\contentsline {subsection}{\numberline {5.2}The Landscape of Empirical Risk}{34}{subsection.0.0.5.2}% 
\contentsline {subsection}{\numberline {5.3}Generalization of Neural Networks}{36}{subsection.0.0.5.3}% 
\contentsline {section}{\numberline {6}Conclusions}{36}{section.0.0.6}% 
\contentsline {section}{Appendix}{38}{section*.15}% 
\contentsline {section}{\numberline {1}Descent Lemma}{38}{section.1..1}% 
\contentsline {section}{\numberline {2}PReLU}{38}{section.1..2}% 
\contentsline {section}{\numberline {3}Numerous Zero-error Minimum}{39}{section.1..3}% 
