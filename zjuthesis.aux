\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@refcontext{none/global//global/global}
\abx@aux@refsection{1}{1}
\HyPL@Entry{0<</P(\376\377\000-\0001)>>}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\newlabel{refsection:1}{{}{-1}{}{Doc-Start}{}}
\HyPL@Entry{1<</S/R>>}
\HyPL@Entry{3<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Network}{2}{section.0.0.2}\protected@file@percent }
\newlabel{sec:NN}{{2}{2}{Neural Network}{section.0.0.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neuron}{2}{subsection.0.0.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The model of neuron\relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neuron}{{2.1}{3}{The model of neuron\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Common activation functions\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:activation}{{2.2}{3}{Common activation functions\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Architecture}{3}{subsection.0.0.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Shallow Neural Network\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:shallowNN}{{2.3}{4}{Shallow Neural Network\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Deep Neural Network\relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:deepNN}{{2.4}{5}{Deep Neural Network\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Loss Function}{5}{subsection.0.0.2.3}\protected@file@percent }
\abx@aux@cite{dauphin2014identifying}
\abx@aux@segm{1}{0}{dauphin2014identifying}
\abx@aux@cite{bertsekas1997nonlinear}
\abx@aux@segm{1}{0}{bertsekas1997nonlinear}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Gradient Descent}{6}{subsection.0.0.2.4}\protected@file@percent }
\newlabel{ssec:GD}{{2.4}{6}{Gradient Descent}{subsection.0.0.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Basic Intuition of GD}{6}{subsubsection.0.0.2.4.1}\protected@file@percent }
\newlabel{equ:GD}{{2-4}{6}{Basic Intuition of GD}{equation.0.0.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Basic Convergence Analysis of GD}{6}{subsubsection.0.0.2.4.2}\protected@file@percent }
\abx@aux@cite{sun2019optimization}
\abx@aux@segm{1}{0}{sun2019optimization}
\newlabel{pro:ConstantConvergence}{{2.1}{7}{}{pro.0.0.2.1}{}}
\newlabel{equ:learningRate}{{2-7}{7}{}{equation.0.0.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Forward Propagation}{8}{subsection.0.0.2.5}\protected@file@percent }
\newlabel{equ:FP}{{2-13}{8}{Forward Propagation}{equation.0.0.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Backward Propagation}{8}{subsection.0.0.2.6}\protected@file@percent }
\newlabel{equ:BP}{{2-15}{9}{Backward Propagation}{equation.0.0.2.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem in Neural Network}{9}{section.0.0.3}\protected@file@percent }
\newlabel{sec:Problem}{{3}{9}{Problem in Neural Network}{section.0.0.3}{}}
\abx@aux@cite{shamir2018exponential}
\abx@aux@segm{1}{0}{shamir2018exponential}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Explosion and Vanishing}{10}{subsection.0.0.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Plot of $ J(w) = (w^7 + 1)^2 $\relax }}{11}{figure.caption.6}\protected@file@percent }
\newlabel{fig:exmp1}{{3.1}{11}{Plot of $ J(w) = (w^7 + 1)^2 $\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Saddle Points in Non-convex Optimization}{11}{subsection.0.0.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Hessian Matrix}{11}{subsubsection.0.0.3.2.1}\protected@file@percent }
\newlabel{def:Hessian}{{3.1}{11}{}{defn.0.0.3.1}{}}
\abx@aux@cite{bray2007statistics}
\abx@aux@segm{1}{0}{bray2007statistics}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}The Prevalence of Saddle Points}{12}{subsubsection.0.0.3.2.2}\protected@file@percent }
\newlabel{sssec:Saddle}{{3.2.2}{12}{The Prevalence of Saddle Points}{subsubsection.0.0.3.2.2}{}}
\abx@aux@segm{1}{0}{dauphin2014identifying}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Different types of critical point\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig:activation}{{3.2}{13}{Different types of critical point\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ReLU: $g(z) = max(0,\ z)$}}}{13}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {sigmoid: $ \sigma (z) = \frac {1}{1+e^{(-z)}} $}}}{13}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {local minimum}}}{13}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {local maximum}}}{13}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {horse saddle point}}}{13}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {monkey saddle point}}}{13}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces (a) and (c) show how critical points are distributed in the $ \epsilon -\alpha $ plane. (b) and (d) plot the distributions of eigenvalues of the Hessian at three different critical points.\relax }}{14}{figure.caption.8}\protected@file@percent }
\newlabel{fig:aevalidation}{{3.3}{14}{(a) and (c) show how critical points are distributed in the $ \epsilon -\alpha $ plane. (b) and (d) plot the distributions of eigenvalues of the Hessian at three different critical points.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Learning Rate Selection}{14}{subsection.0.0.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Goldstein Rule}{14}{subsubsection.0.0.3.3.1}\protected@file@percent }
\abx@aux@cite{mishkin2015all}
\abx@aux@segm{1}{0}{mishkin2015all}
\abx@aux@cite{romero2014fitnets}
\abx@aux@segm{1}{0}{romero2014fitnets}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Armijo Rule}{15}{subsubsection.0.0.3.3.2}\protected@file@percent }
\newlabel{sssec:Armijo}{{3.3.2}{15}{Armijo Rule}{subsubsection.0.0.3.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Diminishing Learning Rate}{15}{subsubsection.0.0.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Tricks in Neural Network}{15}{section.0.0.4}\protected@file@percent }
\newlabel{sec:Solution}{{4}{15}{Tricks in Neural Network}{section.0.0.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}A Good Initialization is All You Need \footnote {The title of this section is quoted from Mishkin et al. \parencite {mishkin2015all} whose title had a profound influence on our study of the neural network.}}{15}{subsection.0.0.4.1}\protected@file@percent }
\abx@aux@cite{glorot2010understanding}
\abx@aux@segm{1}{0}{glorot2010understanding}
\abx@aux@cite{jia2014caffe}
\abx@aux@segm{1}{0}{jia2014caffe}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Xavier Initialization}{16}{subsubsection.0.0.4.1.1}\protected@file@percent }
\newlabel{equ:Xavier}{{4-1}{16}{Xavier Initialization}{equation.0.0.4.1}{}}
\abx@aux@segm{1}{0}{glorot2010understanding}
\abx@aux@cite{he2015delving}
\abx@aux@segm{1}{0}{he2015delving}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Kaiming Initialization}{17}{subsubsection.0.0.4.1.2}\protected@file@percent }
\newlabel{sssec:Kaiming}{{4.1.2}{17}{Kaiming Initialization}{subsubsection.0.0.4.1.2}{}}
\newlabel{equ:Kaiming}{{4-11}{17}{Kaiming Initialization}{equation.0.0.4.11}{}}
\newlabel{equ:Var}{{4-12}{17}{}{equation.0.0.4.12}{}}
\abx@aux@cite{saxe2013exact}
\abx@aux@segm{1}{0}{saxe2013exact}
\abx@aux@segm{1}{0}{saxe2013exact}
\newlabel{equ:Kaiming_1}{{4-15}{18}{}{equation.0.0.4.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Orthogonal Initialization}{18}{subsubsection.0.0.4.1.3}\protected@file@percent }
\abx@aux@segm{1}{0}{mishkin2015all}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The eigenvalue spectrum of a random orthogonal matrix $ W \in \mathbb  {R}^{100\times 100} $\relax }}{19}{figure.caption.9}\protected@file@percent }
\newlabel{fig:orthogonal}{{4.1}{19}{The eigenvalue spectrum of a random orthogonal matrix $ W \in \mathbb {R}^{100\times 100} $\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Layer-Sequential Unit-Variance Initialization (LSUV)}{19}{subsubsection.0.0.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Gradient Descend Variants}{20}{subsection.0.0.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Batch Gradient Descent}{20}{subsubsection.0.0.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Stochastic Gradient Descent (SGD)}{20}{subsubsection.0.0.4.2.2}\protected@file@percent }
\abx@aux@cite{qian1999momentum}
\abx@aux@segm{1}{0}{qian1999momentum}
\abx@aux@cite{momentum}
\abx@aux@segm{1}{0}{momentum}
\abx@aux@segm{1}{0}{momentum}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Mini-batch Gradient Descent}{21}{subsubsection.0.0.4.2.3}\protected@file@percent }
\newlabel{sssec:MiniBatch}{{4.2.3}{21}{Mini-batch Gradient Descent}{subsubsection.0.0.4.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Optimization Algorithms}{21}{subsection.0.0.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Momentum}{21}{subsubsection.0.0.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A long and narrow valley situation. From \parencite {momentum}\relax }}{22}{figure.caption.10}\protected@file@percent }
\newlabel{fig:valley}{{4.2}{22}{A long and narrow valley situation. From \parencite {momentum}\relax }{figure.caption.10}{}}
\newlabel{equ:Momentum}{{4-21}{22}{Momentum}{equation.0.0.4.21}{}}
\newlabel{equ:Newton}{{4-22}{22}{Momentum}{equation.0.0.4.22}{}}
\newlabel{equ:physics}{{4-23}{22}{Momentum}{equation.0.0.4.23}{}}
\abx@aux@cite{burden2010numerical}
\abx@aux@segm{1}{0}{burden2010numerical}
\abx@aux@segm{1}{0}{qian1999momentum}
\abx@aux@cite{nesterov1983method}
\abx@aux@segm{1}{0}{nesterov1983method}
\newlabel{equ:linear}{{4-26}{23}{Momentum}{equation.0.0.4.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Nesterov Accelerated Gradient (NAG)}{23}{subsubsection.0.0.4.3.2}\protected@file@percent }
\newlabel{sssec:NAG}{{4.3.2}{23}{Nesterov Accelerated Gradient (NAG)}{subsubsection.0.0.4.3.2}{}}
\abx@aux@cite{sutskever2013training}
\abx@aux@segm{1}{0}{sutskever2013training}
\abx@aux@cite{rmsprop}
\abx@aux@segm{1}{0}{rmsprop}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces NAG illustration: blue vector represents the momentum method, green vector represents the NAG with the brown component is the gradient and the red component is the correction.\relax }}{24}{figure.caption.11}\protected@file@percent }
\newlabel{fig:Nesterov}{{4.3}{24}{NAG illustration: blue vector represents the momentum method, green vector represents the NAG with the brown component is the gradient and the red component is the correction.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}RMSprop}{24}{subsubsection.0.0.4.3.3}\protected@file@percent }
\newlabel{sssec:RMSprop}{{4.3.3}{24}{RMSprop}{subsubsection.0.0.4.3.3}{}}
\abx@aux@cite{duchi2011adaptive}
\abx@aux@segm{1}{0}{duchi2011adaptive}
\abx@aux@cite{zeiler2012adadelta}
\abx@aux@segm{1}{0}{zeiler2012adadelta}
\abx@aux@segm{1}{0}{burden2010numerical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Adagrad}{25}{subsubsection.0.0.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Adadelta}{25}{subsubsection.0.0.4.3.5}\protected@file@percent }
\abx@aux@cite{kingma2014adam}
\abx@aux@segm{1}{0}{kingma2014adam}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.6}Adam}{26}{subsubsection.0.0.4.3.6}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Adam. Note that all operators are element-wise. The recommended hyperparameters are learning rate $ \alpha = 0.001 $, momentum constant $ \beta = 0.9 $, RMSprop constant $ \gamma = 0.999 $, smoothing term $ \epsilon = 10^{-8} $.\relax }}{27}{algorithm.1}\protected@file@percent }
\newlabel{alg:Adam}{{1}{27}{Adam. Note that all operators are element-wise. The recommended hyperparameters are learning rate $ \alpha = 0.001 $, momentum constant $ \beta = 0.9 $, RMSprop constant $ \gamma = 0.999 $, smoothing term $ \epsilon = 10^{-8} $.\relax }{algorithm.1}{}}
\abx@aux@segm{1}{0}{kingma2014adam}
\abx@aux@segm{1}{0}{kingma2014adam}
\abx@aux@cite{dozat2016incorporating}
\abx@aux@segm{1}{0}{dozat2016incorporating}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.7}AdaMax}{28}{subsubsection.0.0.4.3.7}\protected@file@percent }
\newlabel{equ:AdaMax}{{4-42}{28}{AdaMax}{equation.0.0.4.42}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces AdaMax. Note that all operators are element-wise. The recommended hyperparameters are learning rate $ \alpha = 0.002 $, momentum constant $ \beta = 0.9 $, RMSprop constant $ \gamma = 0.999 $, smoothing term $ \epsilon = 10^{-8} $.\relax }}{29}{algorithm.2}\protected@file@percent }
\newlabel{alg:AdaMax}{{2}{29}{AdaMax. Note that all operators are element-wise. The recommended hyperparameters are learning rate $ \alpha = 0.002 $, momentum constant $ \beta = 0.9 $, RMSprop constant $ \gamma = 0.999 $, smoothing term $ \epsilon = 10^{-8} $.\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.8}NAdam}{30}{subsubsection.0.0.4.3.8}\protected@file@percent }
\abx@aux@cite{poggio2016theory}
\abx@aux@segm{1}{0}{poggio2016theory}
\abx@aux@cite{poggio2017theory}
\abx@aux@segm{1}{0}{poggio2017theory}
\abx@aux@cite{banburski2019theory}
\abx@aux@segm{1}{0}{banburski2019theory}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces NAdam. Note that all operators are element-wise. The recommended hyperparameters are learning rate $ \alpha = 0.002 $, momentum constant $ \beta = 0.975 $, RMSprop constant $ \gamma = 0.999 $, smoothing term $ \epsilon = 10^{-8} $.\relax }}{31}{algorithm.3}\protected@file@percent }
\newlabel{alg:NAdam}{{3}{31}{NAdam. Note that all operators are element-wise. The recommended hyperparameters are learning rate $ \alpha = 0.002 $, momentum constant $ \beta = 0.975 $, RMSprop constant $ \gamma = 0.999 $, smoothing term $ \epsilon = 10^{-8} $.\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Main Questions in Neural Network Theory}{31}{section.0.0.5}\protected@file@percent }
\newlabel{sec:Question}{{5}{31}{Main Questions in Neural Network Theory}{section.0.0.5}{}}
\abx@aux@segm{1}{0}{poggio2016theory}
\abx@aux@cite{mhaskar1996neural}
\abx@aux@segm{1}{0}{mhaskar1996neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Representation Power of Deep Neural Networks}{32}{subsection.0.0.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Definition and Notation}{32}{subsubsection.0.0.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Shallow Networks}{32}{subsubsection.0.0.5.1.2}\protected@file@percent }
\newlabel{thm:Shallow}{{5.1}{32}{}{thm.0.0.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Deep Hierarchy Networks}{33}{subsubsection.0.0.5.1.3}\protected@file@percent }
\abx@aux@segm{1}{0}{poggio2017theory}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The binary tree structure\relax }}{34}{figure.caption.12}\protected@file@percent }
\newlabel{fig:BT}{{5.1}{34}{The binary tree structure\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}The Landscape of Empirical Risk}{34}{subsection.0.0.5.2}\protected@file@percent }
\newlabel{ssec:empirical}{{5.2}{34}{The Landscape of Empirical Risk}{subsection.0.0.5.2}{}}
\newlabel{fig:Basin_1D}{{2(a)}{35}{Subfigure 0 2(a)}{subfigure.0.0.5.2.1}{}}
\newlabel{sub@fig:Basin_1D}{{(a)}{35}{Subfigure 0 2(a)\relax }{subfigure.0.0.5.2.1}{}}
\newlabel{fig:Basin_2D}{{2(b)}{35}{Subfigure 0 2(b)}{subfigure.0.0.5.2.2}{}}
\newlabel{sub@fig:Basin_2D}{{(b)}{35}{Subfigure 0 2(b)\relax }{subfigure.0.0.5.2.2}{}}
\newlabel{fig:Basin_perturbation}{{2(c)}{35}{Subfigure 0 2(c)}{subfigure.0.0.5.2.3}{}}
\newlabel{sub@fig:Basin_perturbation}{{(c)}{35}{Subfigure 0 2(c)\relax }{subfigure.0.0.5.2.3}{}}
\newlabel{fig:Basin_interpolation}{{2(d)}{35}{Subfigure 0 2(d)}{subfigure.0.0.5.2.4}{}}
\newlabel{sub@fig:Basin_interpolation}{{(d)}{35}{Subfigure 0 2(d)\relax }{subfigure.0.0.5.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The basin model\relax }}{35}{figure.caption.13}\protected@file@percent }
\newlabel{fig:Basin}{{5.2}{35}{The basin model\relax }{figure.caption.13}{}}
\abx@aux@segm{1}{0}{banburski2019theory}
\abx@aux@cite{hyperbolic}
\abx@aux@segm{1}{0}{hyperbolic}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The basin-fractal model\relax }}{36}{figure.caption.14}\protected@file@percent }
\newlabel{fig:Fractal}{{5.3}{36}{The basin-fractal model\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Generalization of Neural Networks}{36}{subsection.0.0.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{36}{section.0.0.6}\protected@file@percent }
\abx@aux@segm{1}{0}{he2015delving}
\@writefile{toc}{\contentsline {section}{Appendix}{38}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Descent Lemma}{38}{section.1..1}\protected@file@percent }
\newlabel{app:DL}{{1}{38}{Descent Lemma}{section.1..1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}PReLU}{38}{section.1..2}\protected@file@percent }
\abx@aux@segm{1}{0}{poggio2017theory}
\@writefile{toc}{\contentsline {section}{\numberline {3}Numerous Zero-error Minimum}{39}{section.1..3}\protected@file@percent }
\newlabel{thm:bezout}{{3.1}{39}{}{thm.1..3.1}{}}
